{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 作業 : (Kaggle)鐵達尼生存預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [作業目標]\n",
    "- 試著模仿範例寫法, 在鐵達尼生存預測中, 使用葉編碼並觀察預測效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [作業重點]\n",
    "- 仿造範例, 完成葉編碼的寫作 : 使用隨機森林 (In[3], Out[3], In[4], Out[4])\n",
    "- 仿造範例, 觀察葉編碼搭配邏輯斯迴歸後的效果 (In[5], Out[5], In[6], Out[6]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Pclass                                               Name     Sex   Age  \\\n0       3                            Braund, Mr. Owen Harris    male  22.0   \n1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n2       3                             Heikkinen, Miss. Laina  female  26.0   \n3       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n4       3                           Allen, Mr. William Henry    male  35.0   \n\n   SibSp  Parch            Ticket     Fare Cabin Embarked  \n0      1      0         A/5 21171   7.2500   NaN        S  \n1      1      0          PC 17599  71.2833   C85        C  \n2      0      0  STON/O2. 3101282   7.9250   NaN        S  \n3      1      0            113803  53.1000  C123        S  \n4      0      0            373450   8.0500   NaN        S  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# 做完特徵工程前的所有準備\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 因為擬合(fit)與編碼(transform)需要分開, 因此不使用.get_dummy, 而採用 sklearn 的 OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "data_path = 'C:/Users/user/OneDrive/cupoy/Machine Learning/教材/data/'\n",
    "df = pd.read_csv(data_path + 'titanic_train.csv')\n",
    "\n",
    "train_Y = df['Survived']\n",
    "df = df.drop(['PassengerId', 'Survived'] , axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Pclass      Name  Sex       Age  SibSp  Parch    Ticket      Fare  \\\n0     1.0  0.121348  1.0  0.283951  0.125    0.0  0.769118  0.014151   \n1     0.0  0.213483  0.0  0.481481  0.125    0.0  0.876471  0.139136   \n2     1.0  0.396629  0.0  0.333333  0.000    0.0  0.983824  0.015469   \n3     0.0  0.305618  0.0  0.444444  0.125    0.0  0.072059  0.103644   \n4     1.0  0.016854  1.0  0.444444  0.000    0.0  0.694118  0.015713   \n\n      Cabin  Embarked  \n0  0.000000  1.000000  \n1  0.557823  0.333333  \n2  0.000000  1.000000  \n3  0.380952  1.000000  \n4  0.000000  1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.121348</td>\n      <td>1.0</td>\n      <td>0.283951</td>\n      <td>0.125</td>\n      <td>0.0</td>\n      <td>0.769118</td>\n      <td>0.014151</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.213483</td>\n      <td>0.0</td>\n      <td>0.481481</td>\n      <td>0.125</td>\n      <td>0.0</td>\n      <td>0.876471</td>\n      <td>0.139136</td>\n      <td>0.557823</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.396629</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.983824</td>\n      <td>0.015469</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.305618</td>\n      <td>0.0</td>\n      <td>0.444444</td>\n      <td>0.125</td>\n      <td>0.0</td>\n      <td>0.072059</td>\n      <td>0.103644</td>\n      <td>0.380952</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.016854</td>\n      <td>1.0</td>\n      <td>0.444444</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.694118</td>\n      <td>0.015713</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# 因為需要把類別型與數值型特徵都加入, 故使用最簡版的特徵工程\n",
    "LEncoder = LabelEncoder()\n",
    "MMEncoder = MinMaxScaler()\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].fillna(-1)\n",
    "    if df[c].dtype == 'object':\n",
    "        df[c] = LEncoder.fit_transform(list(df[c].values))\n",
    "    df[c] = MMEncoder.fit_transform(df[c].values.reshape(-1, 1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = df.values\n",
    "# 因為訓練邏輯斯迴歸時也要資料, 因此將訓練及切成三部分 train / val / test, 採用 test 驗證而非 k-fold 交叉驗證\n",
    "# train 用來訓練梯度提升樹, val 用來訓練邏輯斯迴歸, test 驗證效果\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, test_size=0.5)\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 作業1\n",
    "* 請對照範例，完成隨機森林的鐵達尼生存率預測，以及對應的葉編碼+邏輯斯迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 隨機森林擬合後, 再將葉編碼 (*.apply) 結果做獨熱 / 邏輯斯迴歸\n",
    "rf = RandomForestClassifier(n_estimators=20, min_samples_split=10, min_samples_leaf=5, \n",
    "                            max_features=4, max_depth=3, bootstrap=True)\n",
    "onehot = OneHotEncoder()\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "\"\"\"\n",
    "Your Code Here (Hint : 隨機森林的葉編碼(.apply)不需要加上[:, :, 0], 直接用rf.apply()調用即可, 本作業其餘寫法相同)\n",
    "\"\"\"\n",
    "rf.fit(train_X, train_Y)\n",
    "onehot.fit(rf.apply(train_X))\n",
    "lr.fit(onehot.transform(rf.apply(val_X)), val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.54895064 0.18477442 0.58870874 0.21615616 0.16788253 0.1663171\n 0.26211758 0.24539375 0.15612116 0.2976361  0.39758911 0.146462\n 0.78051316 0.82953436 0.17406047 0.59275541 0.72948126 0.75954034\n 0.17301305 0.12947026 0.32943628 0.54022879 0.31320008 0.43689888\n 0.19806359 0.14581529 0.15441107 0.23861918 0.28441648 0.55989348\n 0.13981092 0.66116946 0.1991923  0.28437028 0.17606106 0.17408799\n 0.86113545 0.28364748 0.28441648 0.20037198 0.16602442 0.14913253\n 0.30202299 0.56872788 0.14896089 0.51254782 0.5010129  0.83590771\n 0.21369422 0.82450713 0.31702023 0.17888883 0.62247297 0.29252403\n 0.38525802 0.26964399 0.65475788 0.56723101 0.42643905 0.20262003\n 0.31864842 0.5667522  0.24172314 0.12781845 0.77359402 0.59247957\n 0.24598369 0.13611853 0.17207601 0.44182187 0.5634404  0.16010999\n 0.75553561 0.22494987 0.31864842 0.13981092 0.75415605 0.20359992\n 0.80650899 0.74537612 0.18921886 0.63870402 0.17212408 0.2023329\n 0.79991979 0.30432409 0.27856003 0.6673571  0.14742243 0.46236263\n 0.16602442 0.21770508 0.13118786 0.27635487 0.30202299 0.59019432\n 0.16720954 0.13610845 0.27782516 0.6245514  0.83413032 0.59917891\n 0.3380854  0.56035305 0.79211066 0.562865   0.76503093 0.16476478\n 0.23484793 0.27964636 0.34674728 0.24849547 0.31742705 0.23569124\n 0.80196354 0.42836691 0.77611733 0.29985352 0.23566636 0.26494443\n 0.32847621 0.21349178 0.62965641 0.64890771 0.27856003 0.46953364\n 0.23637319 0.78622194 0.16181577 0.80755559 0.53898554 0.17888883\n 0.3334492  0.2833769  0.80846654 0.29472247 0.61319192 0.82129093\n 0.57625412 0.29071147 0.24242662 0.48422903 0.70469203 0.80915612\n 0.2069309  0.19579749 0.30481978 0.18408168 0.19831455 0.15501248\n 0.54929357 0.82781025 0.2764846  0.19107973 0.27623923 0.16602442\n 0.17278069 0.2833769  0.12259035 0.19279156 0.18330443 0.21134036\n 0.75750399 0.80245169 0.2023329  0.77964103 0.26877167 0.20535361\n 0.59331634 0.28437028 0.2231189  0.31871096 0.26371746 0.70084208\n 0.18370066 0.16616158 0.17232697 0.31792824 0.64848723 0.25922318\n 0.63722689 0.51879068 0.21226574 0.60539192 0.17301305 0.35637708\n 0.23578877 0.16322763 0.14896089 0.165212   0.6540716  0.78803847\n 0.31179675 0.48498168 0.20037198 0.23829335 0.12355078 0.16476478\n 0.5131166  0.14896089 0.30673966 0.59030292 0.1640601  0.29926496\n 0.19869668 0.19843697 0.32365285 0.17301305 0.14613649 0.22542729\n 0.17414629 0.17768164 0.48343322 0.23542888 0.17829033 0.57205531\n 0.35260236 0.17490041 0.68064404 0.26922815 0.25444738 0.56565485\n 0.16602442 0.54024596 0.18650474 0.6972729  0.30187028 0.30432409\n 0.79529194 0.17568475 0.48312271 0.82951882 0.64024596 0.56872788\n 0.79211066 0.30152974 0.30524276 0.17495733 0.2321326  0.21615616\n 0.13118786 0.32752441 0.23325117 0.14707354 0.17233952 0.26922815\n 0.68223822 0.44300898 0.60539192 0.54840232 0.28437028 0.14913253\n 0.63793509 0.79225469 0.79211066 0.17301305 0.27464719 0.23548361\n 0.16200234 0.74770544 0.21770508 0.77412427 0.83666167 0.69799939\n 0.60539192 0.16136206 0.20795581 0.56713999 0.52866411 0.2325667\n 0.15800851 0.80288961 0.16616158 0.79339439 0.79112193 0.22669647\n 0.15501248 0.73453836 0.77049978 0.45759646 0.38017671 0.12526088\n 0.20662719 0.74087015 0.60074424 0.20021245 0.19107973 0.74074521\n 0.1877238  0.22777879 0.40213762 0.17301305 0.17327717 0.80970815\n 0.32836899 0.72004704 0.61009073 0.80687457 0.17301305 0.19542201\n 0.76491303 0.19724996 0.20141765 0.17791251 0.23439639 0.21369422\n 0.30790418 0.16697015 0.15806544 0.19724996 0.30181906 0.22960993\n 0.63548515 0.18330443 0.26508875 0.43847591 0.69064601 0.20108156\n 0.16700814 0.30308122 0.23466409 0.37651069 0.14336166 0.26812349\n 0.68044219 0.28999657 0.75750399 0.18330443 0.27635487 0.79897844\n 0.51813742 0.20762288 0.47707818 0.2639963  0.17791251 0.4128194\n 0.27623923 0.18503034 0.23300834 0.17301305 0.39303085 0.16770004\n 0.65302835 0.23113529 0.84642585 0.57680956 0.22167287 0.27620174\n 0.76329551 0.28999974 0.45992826 0.79991979 0.3460751  0.20330926\n 0.16136206 0.79581271 0.27727574 0.72609705 0.18100006 0.79211066\n 0.18563955 0.82388353 0.5531287  0.56821376 0.25943979 0.17278069\n 0.3576794  0.23378417 0.19843697 0.88670363 0.27494657 0.73200722\n 0.62872792 0.57655986 0.30028191 0.59856915 0.78648242 0.74855809\n 0.78269081 0.54758348 0.74969486 0.17212408 0.17568475 0.14008491\n 0.80068877 0.44793857 0.25580753 0.2231189  0.82722742 0.14914508\n 0.67436117 0.68148954 0.81452942 0.13981092 0.11092656 0.76983447\n 0.19575836 0.34628542 0.29992831 0.67862359 0.27414697 0.18921886\n 0.52333913 0.61679007 0.2023329  0.78320403 0.81115662 0.21101536\n 0.71752607 0.24555668 0.27406996 0.36917972 0.31466727 0.32072457\n 0.82450713 0.14613649 0.14913253 0.17233952 0.32586782 0.3334492\n 0.24980229 0.19279156 0.59339697 0.29385487 0.58969331 0.13647813\n 0.49167777 0.16476478 0.17278069 0.2910567  0.13118786 0.21427417\n 0.18165667 0.15287376 0.3460751  0.1803268  0.37868412 0.16985375\n 0.1952621  0.29439717 0.80189902 0.17408799 0.11429596 0.15045643\n 0.25007416 0.74288411]\n"
    }
   ],
   "source": [
    "# 將隨機森林+葉編碼+邏輯斯迴歸結果輸出\n",
    "\"\"\"\n",
    "Your Code Here \n",
    "\"\"\"\n",
    "rf.apply(train_X)\n",
    "\n",
    "pred_rf_lr = lr.predict_proba(onehot.transform(rf.apply(test_X)))[:, 1]\n",
    "fpr_rf_lr, tpr_rf_lr, _ = roc_curve(test_Y, pred_rf_lr)\n",
    "# 將隨機森林結果輸出\n",
    "\"\"\"\n",
    "Your Code Here \n",
    "\"\"\"\n",
    "pred_rf = rf.predict_proba(test_X)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(test_Y, pred_rf)\n",
    "\n",
    "print(pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業2\n",
    "* 上述的結果，葉編碼是否有提高預測的正確性呢?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 將結果繪圖\n",
    "\"\"\"\n",
    "Your Code Here \n",
    "\"\"\"\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='rf')\n",
    "plt.plot(fpr_rf_lr, tpr_rf_lr, label='rf + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit3aa85e905e81479f963b5b188efc9345"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}